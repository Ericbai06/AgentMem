import json
import os
import time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
from openai import OpenAI
from memos.api.client import MemOSClient
from .config import Config
from .prompts import QUERY_REWRITE_PROMPT, ANSWER_PROMPT

class LocomoAgent:
    def __init__(self):
        # åŒå®¢æˆ·ç«¯åˆå§‹åŒ–
        self.client_origin = MemOSClient(api_key=Config.MEMOS_ORIGIN_API_KEY)
        self.client_process = MemOSClient(api_key=Config.MEMOS_PROCESS_API_KEY)
        
        self.llm_client = OpenAI(api_key=Config.OPENAI_API_KEY, base_url=Config.OPENAI_BASE_URL)
        self.results = defaultdict(list)
        self.output_file = os.path.join(Config.RESULTS_DIR, "final_results.json")

    def rewrite_query(self, question, user_name):
        try:
            prompt = QUERY_REWRITE_PROMPT.format(question=question, user_name=user_name)
            response = self.llm_client.chat.completions.create(
                model=Config.MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0
            )
            return response.choices[0].message.content.strip()
        except:
            return question

    def _parse_search_response(self, res, source_type):
        """é€šç”¨è§£æå‡½æ•°"""
        memories = []
        data_list = []
        # å…¼å®¹ SDK ä¸åŒç‰ˆæœ¬çš„è¿”å›ç»“æ„
        if hasattr(res, 'data') and hasattr(res.data, 'memory_detail_list'):
            data_list = res.data.memory_detail_list
        elif isinstance(res, dict):
            data_list = res.get('data', {}).get('memory_detail_list', [])
        elif isinstance(res, list): # æŸäº›æƒ…å†µç›´æ¥è¿”å› list
            data_list = res
            
        for item in data_list:
            # å¥å£®æ€§è·å–
            content = getattr(item, 'memory_value', None) or (item.get('memory_value') if isinstance(item, dict) else "")
            ts = getattr(item, 'conversation_id', None) or (item.get('conversation_id') if isinstance(item, dict) else "Unknown Date")
            
            if content:
                memories.append({
                    "timestamp": ts, 
                    "content": content, 
                    "source": source_type
                })
        return memories

    def search_memories(self, user_id, query):
        try:
            # å¹¶è¡Œæ£€ç´¢
            with ThreadPoolExecutor(max_workers=2) as executor:
                f1 = executor.submit(self.client_origin.search_memory, query, user_id, "search")
                f2 = executor.submit(self.client_process.search_memory, query, user_id, "search")
                
                res_origin = f1.result()
                res_process = f2.result()
            
            mems_origin = self._parse_search_response(res_origin, "RAW")
            mems_process = self._parse_search_response(res_process, "FACT")
            
            all_memories = mems_origin + mems_process
            # æŒ‰æ—¶é—´æˆ³ç®€å•æ’åº
            all_memories.sort(key=lambda x: str(x.get('timestamp', '')))
            
            return all_memories
        except Exception as e:
            print(f"Search error: {e}")
            return []

    def answer_question(self, user_id, user_name, question):
        # 1. é‡å†™é—®é¢˜
        rewritten_q = self.rewrite_query(question, user_name)
        
        # 2. åŒè·¯æ£€ç´¢
        memories = self.search_memories(user_id, rewritten_q)
        
        # 3. æ„å»ºä¸Šä¸‹æ–‡
        context_lines = []
        for m in memories:
            # æ ¼å¼ï¼š[2022-01-01] Content
            context_lines.append(f"[{m['timestamp']}] {m['content']}")
        
        context_str = "\n".join(context_lines)
        
        # 4. ç”Ÿæˆç­”æ¡ˆ (Direct Answer)
        prompt = ANSWER_PROMPT.format(question=question, context=context_str)
        
        start_t = time.time()
        response = self.llm_client.chat.completions.create(
            model=Config.MODEL_NAME,
            messages=[{"role": "system", "content": prompt}],
            temperature=0.0, # ä¿æŒ0æ¸©ç¡®ä¿ç®€æ´
            max_tokens=50    # é™åˆ¶ token è¾“å‡ºé•¿åº¦ï¼Œå¼ºåˆ¶æ¨¡å‹çŸ­ç­”
        )
        duration = time.time() - start_t
        
        # ç›´æ¥æ‹¿å†…å®¹ï¼Œä¸åšä»»ä½•å¤„ç†
        final_answer = response.choices[0].message.content.strip()
        
        return final_answer, memories, duration

    def process_one_qa(self, qa_item, speaker_a_id, speaker_b_id, spk_a_name, spk_b_name):
        question = qa_item["question"]
        # ç›´æ¥è·å–ç­”æ¡ˆ
        ans_a, mems_a, _ = self.answer_question(speaker_a_id, spk_a_name, question)
        
        return {
            "question": question,
            "answer": qa_item.get("answer", ""),
            "category": qa_item.get("category", ""),
            "response": ans_a, # ç›´æ¥å­˜å…¥æ¨¡å‹è¾“å‡º
            "evidence": [],
            "speaker_1_memories": mems_a,
            "response_time": 0
        }

    def run_eval(self):
        print(f"ğŸš€ Starting Fast-Track Evaluation (Direct Answer)...")
        with open(Config.DATA_PATH, "r") as f:
            raw_data = json.load(f)
            
        for idx, item in tqdm(enumerate(raw_data), total=len(raw_data)):
            spk_a = item["conversation"]["speaker_a"]
            spk_b = item["conversation"]["speaker_b"]
            uid_a = f"{spk_a}_{idx}"
            uid_b = f"{spk_b}_{idx}"
            qa_list = item["qa"]
            
            with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS_SEARCH) as executor:
                futures = []
                for qa in qa_list:
                    futures.append(executor.submit(self.process_one_qa, qa, uid_a, uid_b, spk_a, spk_b))
                
                for f in futures:
                    res = f.result()
                    self.results[idx].append(res)
            
            with open(self.output_file, "w") as f:
                json.dump(self.results, f, indent=4)
        print(f"âœ… Evaluation Complete. Results saved to {self.output_file}")